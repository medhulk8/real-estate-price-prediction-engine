{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Price Prediction Engine - Technical Challenge\n",
    "\n",
    "## Objective\n",
    "Build a production-ready ML model to predict property transaction prices (TRANS_VALUE) with high accuracy and explainability.\n",
    "\n",
    "## Approach Overview\n",
    "1. **EDA**: Understand data quality, distributions, and business patterns\n",
    "2. **Feature Engineering**: Create predictive features while avoiding data leakage\n",
    "3. **Model Development**: Train CatBoost with log-transformed target and MAE loss\n",
    "4. **Evaluation**: Assess performance across price ranges and property segments\n",
    "5. **Interpretability**: Use SHAP to explain predictions and generate business insights\n",
    "\n",
    "## Key Technical Decisions\n",
    "- **Model**: CatBoost (handles categorical features natively, robust to outliers)\n",
    "- **Target Transform**: log1p (handles skewed price distribution)\n",
    "- **Loss Function**: MAE (robust to outliers)\n",
    "- **Split Strategy**: Chronological 70/15/15 (prevents temporal leakage)\n",
    "- **Feature Strategy**: Train-only aggregates with unseen category handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../transactions-2025-03-21.csv')\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column data types and descriptions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COLUMN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "column_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Unique Values': df.nunique()\n",
    "})\n",
    "\n",
    "column_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date column to understand temporal range\n",
    "df['INSTANCE_DATE'] = pd.to_datetime(df['INSTANCE_DATE'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEMPORAL COVERAGE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Date Range: {df['INSTANCE_DATE'].min()} to {df['INSTANCE_DATE'].max()}\")\n",
    "print(f\"Time Span: {(df['INSTANCE_DATE'].max() - df['INSTANCE_DATE'].min()).days} days\")\n",
    "print(f\"\\nTransactions by Year:\")\n",
    "print(df['INSTANCE_DATE'].dt.year.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values and Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed missing value analysis\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Empty_Strings': (df == '').sum(),\n",
    "    'Empty_String_Percentage': ((df == '').sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_analysis = missing_analysis[\n",
    "    (missing_analysis['Missing_Count'] > 0) | (missing_analysis['Empty_Strings'] > 0)\n",
    "].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES & DATA QUALITY ISSUES\")\n",
    "print(\"=\" * 80)\n",
    "if len(missing_analysis) > 0:\n",
    "    display(missing_analysis)\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for empty strings in categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(f\"\\n\\nEmpty string counts in categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    empty_count = (df[col] == '').sum()\n",
    "    if empty_count > 0:\n",
    "        print(f\"{col}: {empty_count} ({empty_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Target Variable Analysis (TRANS_VALUE)\n",
    "\n",
    "Understanding the distribution of our target variable is crucial for:\n",
    "- Choosing appropriate transformations\n",
    "- Identifying outliers\n",
    "- Setting realistic performance expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of TRANS_VALUE\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE (TRANS_VALUE) STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(df['TRANS_VALUE'].describe())\n",
    "print(f\"\\nSkewness: {df['TRANS_VALUE'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['TRANS_VALUE'].kurtosis():.2f}\")\n",
    "\n",
    "# Check for invalid values\n",
    "invalid_prices = (df['TRANS_VALUE'] <= 0).sum()\n",
    "print(f\"\\nInvalid prices (<=0): {invalid_prices} ({invalid_prices/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TRANS_VALUE distribution - Raw vs Log-transformed\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw distribution\n",
    "axes[0, 0].hist(df['TRANS_VALUE'], bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('TRANS_VALUE Distribution (Raw)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Transaction Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df['TRANS_VALUE'].median(), color='red', linestyle='--', label=f'Median: {df[\"TRANS_VALUE\"].median():,.0f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Log-transformed distribution\n",
    "axes[0, 1].hist(np.log1p(df['TRANS_VALUE']), bins=100, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('TRANS_VALUE Distribution (Log1p)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log(Transaction Value + 1)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Boxplot - Raw\n",
    "axes[1, 0].boxplot(df['TRANS_VALUE'], vert=False)\n",
    "axes[1, 0].set_title('TRANS_VALUE Boxplot (Raw)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Transaction Value')\n",
    "\n",
    "# Boxplot - Log\n",
    "axes[1, 1].boxplot(np.log1p(df['TRANS_VALUE']), vert=False)\n",
    "axes[1, 1].set_title('TRANS_VALUE Boxplot (Log1p)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Log(Transaction Value + 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca OBSERVATION: Compare the distributions above.\")\n",
    "print(\"   - Raw distribution is highly right-skewed (long tail of expensive properties)\")\n",
    "print(\"   - Log transformation normalizes the distribution, making it more suitable for modeling\")\n",
    "print(\"   - This justifies using log1p(TRANS_VALUE) as our target during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical columns\n",
    "numerical_cols = ['TRANS_VALUE', 'PROCEDURE_AREA', 'ACTUAL_AREA', 'ROOMS_EN', \n",
    "                  'PARKING', 'TOTAL_BUYER', 'TOTAL_SELLER']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NUMERICAL FEATURES SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "df[numerical_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues in numerical columns\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NUMERICAL DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in ['ACTUAL_AREA', 'PROCEDURE_AREA']:\n",
    "    zero_or_negative = (df[col] <= 0).sum()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Zero or negative values: {zero_or_negative} ({zero_or_negative/len(df)*100:.2f}%)\")\n",
    "    print(f\"  - Min: {df[col].min()}, Max: {df[col].max()}\")\n",
    "    print(f\"  - Median: {df[col].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute price per sqft for analysis (NOT as a model feature to avoid leakage)\n",
    "# This helps us understand the data and identify potential outliers\n",
    "df['price_per_sqft_temp'] = df['TRANS_VALUE'] / df['ACTUAL_AREA'].replace(0, np.nan)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE PER SQFT ANALYSIS (for EDA only, NOT a model feature)\")\n",
    "print(\"=\" * 80)\n",
    "print(df['price_per_sqft_temp'].describe())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df['price_per_sqft_temp'].dropna(), bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Price per Sqft Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Price per Sqft')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].boxplot(df['price_per_sqft_temp'].dropna(), vert=False)\n",
    "axes[1].set_title('Price per Sqft Boxplot', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Price per Sqft')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cardinality of categorical features\n",
    "categorical_cols = ['GROUP_EN', 'PROCEDURE_EN', 'IS_OFFPLAN_EN', 'IS_FREE_HOLD_EN',\n",
    "                   'USAGE_EN', 'AREA_EN', 'PROP_TYPE_EN', 'PROP_SB_TYPE_EN',\n",
    "                   'NEAREST_METRO_EN', 'NEAREST_MALL_EN', 'NEAREST_LANDMARK_EN',\n",
    "                   'MASTER_PROJECT_EN', 'PROJECT_EN']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES CARDINALITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cardinality_df = pd.DataFrame({\n",
    "    'Column': categorical_cols,\n",
    "    'Unique_Values': [df[col].nunique() for col in categorical_cols],\n",
    "    'Cardinality_Ratio': [df[col].nunique() / len(df) for col in categorical_cols]\n",
    "}).sort_values('Unique_Values', ascending=False)\n",
    "\n",
    "cardinality_df['Cardinality_Type'] = cardinality_df['Unique_Values'].apply(\n",
    "    lambda x: 'Low (<10)' if x < 10 else ('Medium (10-100)' if x < 100 else 'High (100+)')\n",
    ")\n",
    "\n",
    "display(cardinality_df)\n",
    "\n",
    "print(\"\\n\ud83d\udcca OBSERVATION:\")\n",
    "print(\"   - High cardinality features (PROJECT_EN, AREA_EN, etc.) need special handling\")\n",
    "print(\"   - CatBoost can handle these natively without one-hot encoding\")\n",
    "print(\"   - We'll create aggregated features from these for additional predictive power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top categories for selected features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# PROP_TYPE_EN\n",
    "top_prop_types = df['PROP_TYPE_EN'].value_counts().head(10)\n",
    "axes[0, 0].barh(range(len(top_prop_types)), top_prop_types.values)\n",
    "axes[0, 0].set_yticks(range(len(top_prop_types)))\n",
    "axes[0, 0].set_yticklabels(top_prop_types.index)\n",
    "axes[0, 0].set_title('Top 10 Property Types', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Count')\n",
    "\n",
    "# AREA_EN\n",
    "top_areas = df['AREA_EN'].value_counts().head(10)\n",
    "axes[0, 1].barh(range(len(top_areas)), top_areas.values)\n",
    "axes[0, 1].set_yticks(range(len(top_areas)))\n",
    "axes[0, 1].set_yticklabels(top_areas.index)\n",
    "axes[0, 1].set_title('Top 10 Areas', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Count')\n",
    "\n",
    "# IS_OFFPLAN_EN\n",
    "offplan_counts = df['IS_OFFPLAN_EN'].value_counts()\n",
    "axes[1, 0].bar(range(len(offplan_counts)), offplan_counts.values)\n",
    "axes[1, 0].set_xticks(range(len(offplan_counts)))\n",
    "axes[1, 0].set_xticklabels(offplan_counts.index)\n",
    "axes[1, 0].set_title('Off-Plan vs Ready Properties', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# IS_FREE_HOLD_EN\n",
    "freehold_counts = df['IS_FREE_HOLD_EN'].value_counts()\n",
    "axes[1, 1].bar(range(len(freehold_counts)), freehold_counts.values)\n",
    "axes[1, 1].set_xticks(range(len(freehold_counts)))\n",
    "axes[1, 1].set_xticklabels(freehold_counts.index)\n",
    "axes[1, 1].set_title('Freehold vs Leasehold Properties', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 PROCEDURE_EN Analysis (Critical for Filtering Decision)\n",
    "\n",
    "**Why this matters:** The challenge asks us to predict market prices, but the dataset contains various transaction types (Sales, Mortgages, Gifts, etc.). We need to determine if non-sale transactions should be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PROCEDURE_EN distribution\n",
    "print(\"=\" * 80)\n",
    "print(\"PROCEDURE_EN (Transaction Type) ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "procedure_counts = df['PROCEDURE_EN'].value_counts()\n",
    "print(\"\\nTransaction Type Distribution:\")\n",
    "print(procedure_counts)\n",
    "print(f\"\\nTotal unique procedure types: {df['PROCEDURE_EN'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TRANS_VALUE by PROCEDURE_EN\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANS_VALUE STATISTICS BY PROCEDURE_EN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "procedure_price_stats = df.groupby('PROCEDURE_EN')['TRANS_VALUE'].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).sort_values('count', ascending=False)\n",
    "\n",
    "display(procedure_price_stats)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count by procedure\n",
    "top_procedures = procedure_counts.head(10)\n",
    "axes[0].barh(range(len(top_procedures)), top_procedures.values)\n",
    "axes[0].set_yticks(range(len(top_procedures)))\n",
    "axes[0].set_yticklabels(top_procedures.index)\n",
    "axes[0].set_title('Top 10 Transaction Types by Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "# Median price by procedure (top 10 by count)\n",
    "top_procedure_names = top_procedures.index\n",
    "median_prices = df[df['PROCEDURE_EN'].isin(top_procedure_names)].groupby('PROCEDURE_EN')['TRANS_VALUE'].median()\n",
    "median_prices = median_prices.reindex(top_procedure_names)\n",
    "axes[1].barh(range(len(median_prices)), median_prices.values, color='green')\n",
    "axes[1].set_yticks(range(len(median_prices)))\n",
    "axes[1].set_yticklabels(median_prices.index)\n",
    "axes[1].set_title('Median Transaction Value by Type', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Median Transaction Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca DECISION POINT:\")\n",
    "print(\"   Based on the analysis above, we need to decide:\")\n",
    "print(\"   - Should we filter to only 'Sale' transactions?\")\n",
    "print(\"   - Or do multiple procedure types represent valid market prices?\")\n",
    "print(\"   - Look for procedure types with similar median prices to Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Outlier Detection and Handling Strategy\n",
    "\n",
    "**Approach:** Conservative, domain-driven outlier removal\n",
    "- Remove only obvious data errors (impossible values)\n",
    "- Keep legitimate luxury properties\n",
    "- Use robust loss (MAE) to reduce sensitivity to remaining outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential outliers based on domain rules\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (Domain-Driven Rules)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rule 1: Invalid areas (<=0)\n",
    "invalid_actual_area = (df['ACTUAL_AREA'] <= 0).sum()\n",
    "invalid_procedure_area = (df['PROCEDURE_AREA'] <= 0).sum()\n",
    "print(f\"\\nRule 1 - Invalid Areas:\")\n",
    "print(f\"  ACTUAL_AREA <= 0: {invalid_actual_area} rows ({invalid_actual_area/len(df)*100:.2f}%)\")\n",
    "print(f\"  PROCEDURE_AREA <= 0: {invalid_procedure_area} rows ({invalid_procedure_area/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Rule 2: Invalid transaction values (<=0)\n",
    "invalid_trans_value = (df['TRANS_VALUE'] <= 0).sum()\n",
    "print(f\"\\nRule 2 - Invalid Transaction Values:\")\n",
    "print(f\"  TRANS_VALUE <= 0: {invalid_trans_value} rows ({invalid_trans_value/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Rule 3: Extreme area values (will define thresholds based on percentiles)\n",
    "area_p01 = df['ACTUAL_AREA'].quantile(0.001)\n",
    "area_p99 = df['ACTUAL_AREA'].quantile(0.999)\n",
    "print(f\"\\nRule 3 - Extreme Area Values:\")\n",
    "print(f\"  0.1th percentile: {area_p01:.2f}\")\n",
    "print(f\"  99.9th percentile: {area_p99:.2f}\")\n",
    "extreme_small_area = (df['ACTUAL_AREA'] < area_p01).sum()\n",
    "extreme_large_area = (df['ACTUAL_AREA'] > area_p99).sum()\n",
    "print(f\"  Areas < {area_p01:.2f}: {extreme_small_area} rows\")\n",
    "print(f\"  Areas > {area_p99:.2f}: {extreme_large_area} rows\")\n",
    "\n",
    "# Rule 4: Extreme price-per-area ratios\n",
    "price_per_area_p01 = df['price_per_sqft_temp'].quantile(0.001)\n",
    "price_per_area_p99 = df['price_per_sqft_temp'].quantile(0.999)\n",
    "print(f\"\\nRule 4 - Extreme Price-per-Sqft Ratios:\")\n",
    "print(f\"  0.1th percentile: {price_per_area_p01:.2f}\")\n",
    "print(f\"  99.9th percentile: {price_per_area_p99:.2f}\")\n",
    "extreme_low_price_ratio = (df['price_per_sqft_temp'] < price_per_area_p01).sum()\n",
    "extreme_high_price_ratio = (df['price_per_sqft_temp'] > price_per_area_p99).sum()\n",
    "print(f\"  Price/sqft < {price_per_area_p01:.2f}: {extreme_low_price_ratio} rows\")\n",
    "print(f\"  Price/sqft > {price_per_area_p99:.2f}: {extreme_high_price_ratio} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Key Business Insights\n",
    "\n",
    "This section addresses the challenge requirements for insights on:\n",
    "- Price trends by property type, area, and time\n",
    "- Impact of location features\n",
    "- Off-plan vs ready property pricing\n",
    "- Freehold vs leasehold impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8.1 Price Trends by Property Type\n",
    "print(\"=\" * 80)\n",
    "print(\"PRICE TRENDS BY PROPERTY TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prop_type_stats = df.groupby('PROP_TYPE_EN').agg({\n",
    "    'TRANS_VALUE': ['count', 'mean', 'median'],\n",
    "    'ACTUAL_AREA': 'median'\n",
    "}).round(2)\n",
    "\n",
    "prop_type_stats.columns = ['Count', 'Mean_Price', 'Median_Price', 'Median_Area']\n",
    "prop_type_stats = prop_type_stats.sort_values('Count', ascending=False).head(10)\n",
    "\n",
    "display(prop_type_stats)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].barh(range(len(prop_type_stats)), prop_type_stats['Median_Price'])\n",
    "axes[0].set_yticks(range(len(prop_type_stats)))\n",
    "axes[0].set_yticklabels(prop_type_stats.index)\n",
    "axes[0].set_title('Median Price by Property Type (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Median Transaction Value')\n",
    "\n",
    "axes[1].barh(range(len(prop_type_stats)), prop_type_stats['Median_Area'], color='green')\n",
    "axes[1].set_yticks(range(len(prop_type_stats)))\n",
    "axes[1].set_yticklabels(prop_type_stats.index)\n",
    "axes[1].set_title('Median Area by Property Type (Top 10)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Median Area')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8.2 Price Trends by Area\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE TRENDS BY AREA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "area_stats = df.groupby('AREA_EN').agg({\n",
    "    'TRANS_VALUE': ['count', 'mean', 'median'],\n",
    "    'ACTUAL_AREA': 'median'\n",
    "}).round(2)\n",
    "\n",
    "area_stats.columns = ['Count', 'Mean_Price', 'Median_Price', 'Median_Area']\n",
    "area_stats = area_stats[area_stats['Count'] >= 50].sort_values('Median_Price', ascending=False).head(15)\n",
    "\n",
    "display(area_stats)\n",
    "\n",
    "# Visualize top premium areas\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(range(len(area_stats)), area_stats['Median_Price'])\n",
    "plt.yticks(range(len(area_stats)), area_stats.index)\n",
    "plt.title('Top 15 Premium Areas by Median Price (min 50 transactions)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Median Transaction Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca INSIGHT: Location premium is significant - top areas command much higher prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8.3 Price Trends Over Time\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE TRENDS OVER TIME\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Monthly median prices\n",
    "df['year_month'] = df['INSTANCE_DATE'].dt.to_period('M')\n",
    "monthly_prices = df.groupby('year_month')['TRANS_VALUE'].agg(['median', 'count'])\n",
    "\n",
    "# Quarterly median prices\n",
    "df['year_quarter'] = df['INSTANCE_DATE'].dt.to_period('Q')\n",
    "quarterly_prices = df.groupby('year_quarter')['TRANS_VALUE'].agg(['median', 'count'])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Monthly trend\n",
    "axes[0].plot(monthly_prices.index.astype(str), monthly_prices['median'], marker='o', linewidth=2)\n",
    "axes[0].set_title('Median Transaction Price - Monthly Trend', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Median Price')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Quarterly trend\n",
    "axes[1].plot(quarterly_prices.index.astype(str), quarterly_prices['median'], marker='s', linewidth=2, color='green')\n",
    "axes[1].set_title('Median Transaction Price - Quarterly Trend', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Quarter')\n",
    "axes[1].set_ylabel('Median Price')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca INSIGHT: Temporal patterns visible - justifies including time-based features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8.4 Off-Plan vs Ready Properties\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OFF-PLAN VS READY PROPERTIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "offplan_comparison = df.groupby('IS_OFFPLAN_EN')['TRANS_VALUE'].agg([\n",
    "    'count', 'mean', 'median', 'std'\n",
    "]).round(2)\n",
    "\n",
    "display(offplan_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='TRANS_VALUE', by='IS_OFFPLAN_EN', ax=axes[0])\n",
    "axes[0].set_title('Transaction Value: Off-Plan vs Ready', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Property Status')\n",
    "axes[0].set_ylabel('Transaction Value')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Bar plot of medians\n",
    "axes[1].bar(range(len(offplan_comparison)), offplan_comparison['median'])\n",
    "axes[1].set_xticks(range(len(offplan_comparison)))\n",
    "axes[1].set_xticklabels(offplan_comparison.index)\n",
    "axes[1].set_title('Median Price: Off-Plan vs Ready', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Median Transaction Value')\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "price_diff_pct = (offplan_comparison.loc['Off-Plan', 'median'] / offplan_comparison.loc['Ready', 'median'] - 1) * 100 if 'Ready' in offplan_comparison.index and 'Off-Plan' in offplan_comparison.index else 0\n",
    "print(f\"\\n\ud83d\udcca INSIGHT: Off-plan properties are {abs(price_diff_pct):.1f}% {'more' if price_diff_pct > 0 else 'less'} expensive than ready properties (median)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8.5 Freehold vs Leasehold Impact\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FREEHOLD VS LEASEHOLD PROPERTIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "freehold_comparison = df.groupby('IS_FREE_HOLD_EN')['TRANS_VALUE'].agg([\n",
    "    'count', 'mean', 'median', 'std'\n",
    "]).round(2)\n",
    "\n",
    "display(freehold_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='TRANS_VALUE', by='IS_FREE_HOLD_EN', ax=axes[0])\n",
    "axes[0].set_title('Transaction Value: Freehold vs Leasehold', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Ownership Type')\n",
    "axes[0].set_ylabel('Transaction Value')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Bar plot of medians\n",
    "axes[1].bar(range(len(freehold_comparison)), freehold_comparison['median'], color='green')\n",
    "axes[1].set_xticks(range(len(freehold_comparison)))\n",
    "axes[1].set_xticklabels(freehold_comparison.index)\n",
    "axes[1].set_title('Median Price: Freehold vs Leasehold', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Median Transaction Value')\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "price_diff_pct_fh = (freehold_comparison.loc['Free Hold', 'median'] / freehold_comparison.loc['Lease Hold', 'median'] - 1) * 100 if 'Lease Hold' in freehold_comparison.index and 'Free Hold' in freehold_comparison.index else 0\n",
    "print(f\"\\n\ud83d\udcca INSIGHT: Freehold properties are {abs(price_diff_pct_fh):.1f}% {'more' if price_diff_pct_fh > 0 else 'less'} expensive than leasehold properties (median)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning and Preparation\n",
    "\n",
    "Based on EDA insights, we'll now:\n",
    "1. Filter dataset based on PROCEDURE_EN analysis\n",
    "2. Remove obvious data errors (outliers)\n",
    "3. Prepare data for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original dataset size\n",
    "original_size = len(df)\n",
    "print(f\"Original dataset size: {original_size:,} rows\")\n",
    "\n",
    "# TODO: Based on PROCEDURE_EN analysis above, decide filtering strategy\n",
    "# For now, we'll create a placeholder that will be filled after EDA analysis\n",
    "# Example: df_clean = df[df['PROCEDURE_EN'] == 'Sales'].copy()\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f  DECISION REQUIRED: Review PROCEDURE_EN analysis above to determine filtering strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Status: EDA Framework Complete \u2713\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run this notebook and analyze results\n",
    "2. Make decisions on PROCEDURE_EN filtering\n",
    "3. Define exact outlier removal rules\n",
    "4. Proceed to feature engineering section\n",
    "\n",
    "**The notebook will continue with:**\n",
    "- Feature Engineering\n",
    "- Model Training\n",
    "- Evaluation & Error Analysis\n",
    "- SHAP Interpretability\n",
    "\n",
    "This will be added in the next iteration after we analyze the EDA results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 EDA Conclusions and Decisions\n",
    "\n",
    "Based on the EDA above, we make the following decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EDA CONCLUSIONS & PREPROCESSING DECISIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. PROCEDURE_EN Filtering:\")\n",
    "print(\"   Decision: Based on median price analysis, we should filter to keep only\")\n",
    "print(\"   transaction types that represent true market sales.\")\n",
    "print(\"   Implementation: Will be handled by preprocessing.filter_by_procedure()\")\n",
    "\n",
    "# Identify sale-like procedures (those with median prices similar to actual sales)\n",
    "# Look for procedures with 'Sale' or 'Sales' in the name\n",
    "sale_procedures = df['PROCEDURE_EN'].unique()\n",
    "sale_procedures_list = [p for p in sale_procedures if 'Sale' in p or 'sales' in p.lower()]\n",
    "print(f\"\\n   Candidate sale procedures: {sale_procedures_list}\")\n",
    "\n",
    "# If we find sale procedures, use them; otherwise keep all (decision to be made after inspection)\n",
    "if len(sale_procedures_list) > 0:\n",
    "    KEEP_PROCEDURES = sale_procedures_list\n",
    "    print(f\"   \u2713 Will filter to: {KEEP_PROCEDURES}\")\n",
    "else:\n",
    "    KEEP_PROCEDURES = None\n",
    "    print(\"   \u26a0\ufe0f  No explicit 'Sale' procedures found - review required\")\n",
    "\n",
    "print(\"\\n2. Outlier Removal:\")\n",
    "print(\"   Conservative approach - remove only obvious data errors:\")\n",
    "print(\"   - ACTUAL_AREA <= 0\")\n",
    "print(\"   - PROCEDURE_AREA <= 0\")\n",
    "print(\"   - TRANS_VALUE <= 0\")\n",
    "print(\"   - Extreme price-per-area outliers (0.1-99.9 percentiles)\")\n",
    "\n",
    "print(\"\\n3. Area Unit Determination:\")\n",
    "# Determine if areas are in sqm or sqft based on typical values\n",
    "median_area = df['ACTUAL_AREA'].median()\n",
    "if median_area < 500:  # Likely sqm (apartments typically 50-200 sqm)\n",
    "    AREA_UNIT = 'sqm'\n",
    "else:  # Likely sqft (apartments typically 500-2000 sqft)\n",
    "    AREA_UNIT = 'sqft'\n",
    "print(f\"   Based on median area of {median_area:.0f}, unit is: {AREA_UNIT}\")\n",
    "\n",
    "print(\"\\n4. Target Transform:\")\n",
    "print(\"   log1p(TRANS_VALUE) - justified by right-skewed distribution\")\n",
    "\n",
    "print(\"\\n5. Feature Engineering Strategy:\")\n",
    "print(\"   - Time features from INSTANCE_DATE\")\n",
    "print(\"   - Ratio features (area_ratio, rooms_density)\")\n",
    "print(\"   - Train-only aggregates for AREA/PROJECT/MASTER_PROJECT\")\n",
    "print(\"   - CatBoost native categorical handling\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering & Preprocessing Pipeline\n",
    "\n",
    "Now we'll use our production preprocessing module to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our preprocessing module\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from preprocessing import run_full_preprocessing_pipeline\n",
    "\n",
    "print(\"Preprocessing module imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete preprocessing pipeline\n",
    "# This will:\n",
    "# 1. Filter by procedure type\n",
    "# 2. Remove data errors\n",
    "# 3. Split chronologically (70/15/15)\n",
    "# 4. Handle missing values\n",
    "# 5. Create time features\n",
    "# 6. Create ratio features\n",
    "# 7. Create aggregate features (train-only stats)\n",
    "# 8. Prepare final X, y matrices\n",
    "\n",
    "preprocessing_results = run_full_preprocessing_pipeline(\n",
    "    df=df,\n",
    "    keep_procedures=KEEP_PROCEDURES,\n",
    "    train_ratio=0.70,\n",
    "    val_ratio=0.15\n",
    ")\n",
    "\n",
    "# Extract results\n",
    "X_train = preprocessing_results['X_train']\n",
    "y_train = preprocessing_results['y_train']\n",
    "X_val = preprocessing_results['X_val']\n",
    "y_val = preprocessing_results['y_val']\n",
    "X_test = preprocessing_results['X_test']\n",
    "y_test = preprocessing_results['y_test']\n",
    "preprocessing_metadata = preprocessing_results['preprocessing_metadata']\n",
    "categorical_indices = preprocessing_results['categorical_indices']\n",
    "\n",
    "print(\"\\n\u2713 Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect feature matrix\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE MATRIX SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFeature columns ({len(X_train.columns)}):\")\n",
    "print(X_train.columns.tolist())\n",
    "\n",
    "print(f\"\\n\\nFeature data types:\")\n",
    "print(X_train.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n\\nCategorical features ({len(categorical_indices)}):\")\n",
    "cat_cols = [X_train.columns[i] for i in categorical_indices]\n",
    "print(cat_cols)\n",
    "\n",
    "print(f\"\\n\\nFirst few rows of feature matrix:\")\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering Verification\n",
    "\n",
    "Let's verify that our key features were created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check time features\n",
    "print(\"Time Features Created:\")\n",
    "time_features = ['year', 'month', 'quarter', 'days_since_start', 'transaction_age_days']\n",
    "for feat in time_features:\n",
    "    if feat in X_train.columns:\n",
    "        print(f\"  \u2713 {feat}: range {X_train[feat].min()} to {X_train[feat].max()}\")\n",
    "\n",
    "# Check ratio features\n",
    "print(\"\\nRatio Features Created:\")\n",
    "ratio_features = ['area_ratio', 'rooms_density']\n",
    "for feat in ratio_features:\n",
    "    if feat in X_train.columns:\n",
    "        print(f\"  \u2713 {feat}: mean {X_train[feat].mean():.3f}, std {X_train[feat].std():.3f}\")\n",
    "\n",
    "# Check aggregate features\n",
    "print(\"\\nAggregate Features Created (examples):\")\n",
    "agg_features = [c for c in X_train.columns if '_median_price' in c or '_txn_count' in c]\n",
    "for feat in agg_features[:6]:  # Show first 6\n",
    "    print(f\"  \u2713 {feat}\")\n",
    "if len(agg_features) > 6:\n",
    "    print(f\"  ... and {len(agg_features) - 6} more\")\n",
    "\n",
    "# Check unseen flags\n",
    "print(\"\\nUnseen Category Flags:\")\n",
    "unseen_flags = [c for c in X_train.columns if 'is_unseen' in c]\n",
    "for flag in unseen_flags:\n",
    "    print(f\"  \u2713 {flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training\n",
    "\n",
    "Now we train our CatBoost model with optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model training functions\n",
    "from model import (\n",
    "    train_catboost_model,\n",
    "    evaluate_model,\n",
    "    compute_confidence_intervals,\n",
    "    get_feature_importance,\n",
    "    save_model_artifact,\n",
    "    evaluate_by_price_buckets,\n",
    "    evaluate_by_category\n",
    ")\n",
    "\n",
    "print(\"Model training functions imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost model\n",
    "model = train_catboost_model(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    categorical_indices=categorical_indices,\n",
    "    random_seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all splits\n",
    "metrics = evaluate_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df['Dataset'] = metrics_df.index\n",
    "metrics_df = metrics_df[['Dataset', 'r2', 'mae', 'mape']]\n",
    "metrics_df.columns = ['Dataset', 'R\u00b2 Score', 'MAE', 'MAPE (%)']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"\\n\ud83d\udcca KEY INSIGHTS:\")\n",
    "print(f\"   - Validation R\u00b2 = {metrics['val']['r2']:.4f} \u2192 Model explains {metrics['val']['r2']*100:.1f}% of price variance\")\n",
    "print(f\"   - Validation MAE = {metrics['val']['mae']:,.0f} \u2192 Average error magnitude\")\n",
    "print(f\"   - Test R\u00b2 = {metrics['test']['r2']:.4f} \u2192 Held-out performance (final metric)\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_val_gap = metrics['train']['r2'] - metrics['val']['r2']\n",
    "if train_val_gap < 0.05:\n",
    "    print(f\"   \u2713 No overfitting detected (train-val R\u00b2 gap = {train_val_gap:.4f})\")\n",
    "elif train_val_gap < 0.10:\n",
    "    print(f\"   \u26a0\ufe0f  Slight overfitting (train-val R\u00b2 gap = {train_val_gap:.4f})\")\n",
    "else:\n",
    "    print(f\"   \u274c Overfitting detected (train-val R\u00b2 gap = {train_val_gap:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = get_feature_importance(model, X_train, top_n=20)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "display(feature_importance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 20 Feature Importances (CatBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca BUSINESS INSIGHTS FROM FEATURE IMPORTANCE:\")\n",
    "top_3 = feature_importance.head(3)\n",
    "for idx, row in top_3.iterrows():\n",
    "    feature_name = row['feature']\n",
    "    if 'ACTUAL_AREA' in feature_name:\n",
    "        print(f\"   - {feature_name}: Property size is the strongest predictor\")\n",
    "    elif 'median_price' in feature_name:\n",
    "        print(f\"   - {feature_name}: Location premium captured via aggregates\")\n",
    "    elif 'AREA_EN' in feature_name or 'PROJECT' in feature_name:\n",
    "        print(f\"   - {feature_name}: Location is highly predictive\")\n",
    "    elif 'time' in feature_name.lower() or 'year' in feature_name or 'days' in feature_name:\n",
    "        print(f\"   - {feature_name}: Temporal patterns matter\")\n",
    "    else:\n",
    "        print(f\"   - {feature_name}: Important predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Price Range-Wise Performance (Required by Challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for validation set (for error analysis)\n",
    "y_val_pred_log = model.predict(X_val)\n",
    "y_val_true = np.expm1(y_val)\n",
    "y_val_pred = np.expm1(y_val_pred_log)\n",
    "\n",
    "# Evaluate by price buckets\n",
    "price_bucket_performance = evaluate_by_price_buckets(y_val_true, y_val_pred, n_buckets=5)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE BY PRICE QUANTILES (Validation Set)\")\n",
    "print(\"=\" * 80)\n",
    "display(price_bucket_performance)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# R\u00b2 by bucket\n",
    "axes[0].bar(range(len(price_bucket_performance)), price_bucket_performance['r2'])\n",
    "axes[0].set_xticks(range(len(price_bucket_performance)))\n",
    "axes[0].set_xticklabels(price_bucket_performance['bucket'])\n",
    "axes[0].set_ylabel('R\u00b2 Score')\n",
    "axes[0].set_title('R\u00b2 by Price Bucket', fontweight='bold')\n",
    "axes[0].axhline(y=0.8, color='r', linestyle='--', label='Target 0.8')\n",
    "axes[0].legend()\n",
    "\n",
    "# MAE by bucket\n",
    "axes[1].bar(range(len(price_bucket_performance)), price_bucket_performance['mae'], color='green')\n",
    "axes[1].set_xticks(range(len(price_bucket_performance)))\n",
    "axes[1].set_xticklabels(price_bucket_performance['bucket'])\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE by Price Bucket', fontweight='bold')\n",
    "\n",
    "# MAPE by bucket\n",
    "axes[2].bar(range(len(price_bucket_performance)), price_bucket_performance['mape'], color='orange')\n",
    "axes[2].set_xticks(range(len(price_bucket_performance)))\n",
    "axes[2].set_xticklabels(price_bucket_performance['bucket'])\n",
    "axes[2].set_ylabel('MAPE (%)')\n",
    "axes[2].set_title('MAPE by Price Bucket', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca INSIGHTS:\")\n",
    "print(\"   - Lower price properties: More predictable (higher R\u00b2, lower MAPE)\")\n",
    "print(\"   - Higher price properties: More variance (lower R\u00b2, higher MAE)\")\n",
    "print(\"   - This is expected: luxury properties have more unique features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Performance by Property Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the original categorical features for segmented analysis\n",
    "# These were dropped during prepare_features_and_target, so we'll reload them\n",
    "\n",
    "# Re-run preprocessing but keep the intermediate DataFrames\n",
    "print(\"Loading data for segmented analysis...\")\n",
    "\n",
    "# Quick reload with same settings\n",
    "from preprocessing import (\n",
    "    parse_dates,\n",
    "    filter_by_procedure,\n",
    "    remove_data_errors,\n",
    "    chronological_split\n",
    ")\n",
    "\n",
    "df_reload = pd.read_csv('../transactions-2025-03-21.csv')\n",
    "df_reload = parse_dates(df_reload)\n",
    "if KEEP_PROCEDURES is not None:\n",
    "    df_reload = filter_by_procedure(df_reload, KEEP_PROCEDURES)\n",
    "df_reload = remove_data_errors(df_reload, verbose=False)\n",
    "train_df, val_df, test_df = chronological_split(df_reload)\n",
    "\n",
    "# Now we have the validation DataFrame with categorical columns\n",
    "print(f\"\u2713 Reloaded {len(val_df):,} validation rows for segmented analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Area (top 10 areas by count)\n",
    "area_performance = evaluate_by_category(\n",
    "    y_true=y_val_true,\n",
    "    y_pred=y_val_pred,\n",
    "    category=val_df['AREA_EN'].reset_index(drop=True),\n",
    "    category_name='AREA_EN',\n",
    "    min_samples=30\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE BY AREA (Top 10, min 30 samples)\")\n",
    "print(\"=\" * 80)\n",
    "display(area_performance.head(10))\n",
    "\n",
    "# Visualize\n",
    "top_areas = area_performance.head(10)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].barh(range(len(top_areas)), top_areas['r2'])\n",
    "axes[0].set_yticks(range(len(top_areas)))\n",
    "axes[0].set_yticklabels(top_areas['category'])\n",
    "axes[0].set_xlabel('R\u00b2 Score')\n",
    "axes[0].set_title('R\u00b2 by Area (Top 10)', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(range(len(top_areas)), top_areas['mae'], color='green')\n",
    "axes[1].set_yticks(range(len(top_areas)))\n",
    "axes[1].set_yticklabels(top_areas['category'])\n",
    "axes[1].set_xlabel('MAE')\n",
    "axes[1].set_title('MAE by Area (Top 10)', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Property Type\n",
    "prop_type_performance = evaluate_by_category(\n",
    "    y_true=y_val_true,\n",
    "    y_pred=y_val_pred,\n",
    "    category=val_df['PROP_TYPE_EN'].reset_index(drop=True),\n",
    "    category_name='PROP_TYPE_EN',\n",
    "    min_samples=30\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE BY PROPERTY TYPE\")\n",
    "print(\"=\" * 80)\n",
    "display(prop_type_performance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(prop_type_performance)), prop_type_performance['r2'])\n",
    "plt.yticks(range(len(prop_type_performance)), prop_type_performance['category'])\n",
    "plt.xlabel('R\u00b2 Score')\n",
    "plt.title('R\u00b2 by Property Type', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance: Off-Plan vs Ready\n",
    "offplan_performance = evaluate_by_category(\n",
    "    y_true=y_val_true,\n",
    "    y_pred=y_val_pred,\n",
    "    category=val_df['IS_OFFPLAN_EN'].reset_index(drop=True),\n",
    "    category_name='IS_OFFPLAN_EN',\n",
    "    min_samples=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE: OFF-PLAN VS READY\")\n",
    "print(\"=\" * 80)\n",
    "display(offplan_performance)\n",
    "\n",
    "# Performance: Freehold vs Leasehold\n",
    "freehold_performance = evaluate_by_category(\n",
    "    y_true=y_val_true,\n",
    "    y_pred=y_val_pred,\n",
    "    category=val_df['IS_FREE_HOLD_EN'].reset_index(drop=True),\n",
    "    category_name='IS_FREE_HOLD_EN',\n",
    "    min_samples=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE: FREEHOLD VS LEASEHOLD\")\n",
    "print(\"=\" * 80)\n",
    "display(freehold_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Error Analysis\n",
    "\n",
    "Identify where the model makes the largest errors and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = y_val_true - y_val_pred\n",
    "abs_residuals = np.abs(residuals)\n",
    "pct_errors = (abs_residuals / y_val_true) * 100\n",
    "\n",
    "# Create error analysis DataFrame\n",
    "error_df = pd.DataFrame({\n",
    "    'true_price': y_val_true,\n",
    "    'pred_price': y_val_pred,\n",
    "    'residual': residuals,\n",
    "    'abs_residual': abs_residuals,\n",
    "    'pct_error': pct_errors,\n",
    "    'area': val_df['AREA_EN'].reset_index(drop=True),\n",
    "    'prop_type': val_df['PROP_TYPE_EN'].reset_index(drop=True),\n",
    "    'actual_area': val_df['ACTUAL_AREA'].reset_index(drop=True)\n",
    "})\n",
    "\n",
    "# Find worst predictions\n",
    "worst_predictions = error_df.nlargest(10, 'abs_residual')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 WORST PREDICTIONS (Largest Absolute Errors)\")\n",
    "print(\"=\" * 80)\n",
    "display(worst_predictions[['true_price', 'pred_price', 'abs_residual', 'pct_error', 'area', 'prop_type', 'actual_area']])\n",
    "\n",
    "print(\"\\n\ud83d\udcca ERROR ANALYSIS INSIGHTS:\")\n",
    "print(f\"   - Largest errors occur in: {worst_predictions['area'].mode()[0] if len(worst_predictions) > 0 else 'N/A'}\")\n",
    "print(f\"   - Median % error: {pct_errors.median():.2f}%\")\n",
    "print(f\"   - 90th percentile % error: {pct_errors.quantile(0.90):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_val_true, y_val_pred, alpha=0.3, s=10)\n",
    "axes[0, 0].plot([y_val_true.min(), y_val_true.max()], \n",
    "                [y_val_true.min(), y_val_true.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('True Price', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Predicted Price', fontsize=11)\n",
    "axes[0, 0].set_title('Predicted vs Actual Prices', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0, 1].scatter(y_val_pred, residuals, alpha=0.3, s=10)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Price', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Residual', fontsize=11)\n",
    "axes[0, 1].set_title('Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Residual distribution\n",
    "axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residual', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. Absolute Error by Price Range\n",
    "price_bins = pd.qcut(y_val_true, q=10, duplicates='drop')\n",
    "error_by_price = error_df.groupby(price_bins)['abs_residual'].median().sort_index()\n",
    "axes[1, 1].plot(range(len(error_by_price)), error_by_price.values, marker='o', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Price Decile', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Median Absolute Error', fontsize=11)\n",
    "axes[1, 1].set_title('Error Increases with Price', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca RESIDUAL PLOT INSIGHTS:\")\n",
    "print(\"   - Top-left: Points should cluster around red line (perfect prediction)\")\n",
    "print(\"   - Top-right: Residuals should be randomly scattered around 0 (no pattern)\")\n",
    "print(\"   - Bottom-left: Residuals should be normally distributed around 0\")\n",
    "print(\"   - Bottom-right: Error magnitude increases with price (heteroscedasticity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Compute Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CI statistics from validation residuals\n",
    "ci_stats = compute_confidence_intervals(\n",
    "    model=model,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    n_quantiles=3,\n",
    "    quantile_levels=(0.05, 0.95)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Interpretability with SHAP\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) shows:\n",
    "- Which features drive predictions\n",
    "- How each feature impacts specific predictions\n",
    "- Business insights for stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "print(\"Computing SHAP values... (this may take 2-3 minutes)\")\n",
    "\n",
    "# Use a sample of validation data for SHAP (full dataset too slow)\n",
    "shap_sample_size = min(1000, len(X_val))\n",
    "X_val_sample = X_val.sample(n=shap_sample_size, random_state=RANDOM_SEED)\n",
    "\n",
    "# Create SHAP explainer for tree-based models\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val_sample)\n",
    "\n",
    "print(f\"\u2713 SHAP values computed for {shap_sample_size} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Global Feature Importance (SHAP Summary Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot - shows feature importance AND direction of impact\n",
    "print(\"=\" * 80)\n",
    "print(\"SHAP SUMMARY PLOT - Global Feature Importance\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nHow to read:\")\n",
    "print(\"  - Y-axis: Features ranked by importance (top = most important)\")\n",
    "print(\"  - X-axis: SHAP value (impact on prediction in log-space)\")\n",
    "print(\"  - Color: Feature value (red = high, blue = low)\")\n",
    "print(\"  - Each dot: One property\")\n",
    "print(\"\\nExample interpretation:\")\n",
    "print(\"  If ACTUAL_AREA is top feature with red dots on right:\")\n",
    "print(\"  \u2192 Larger areas (red) push predictions higher (positive SHAP)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_val_sample, max_display=20, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot - simple feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_val_sample, plot_type=\"bar\", max_display=15, show=False)\n",
    "plt.title('Top 15 Features by Mean Absolute SHAP Value', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Local Explanations - Individual Predictions\n",
    "\n",
    "Let's explain 3 specific predictions to show how the model reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3 representative examples: low, medium, high price\n",
    "y_val_true_sample = np.expm1(y_val.loc[X_val_sample.index])\n",
    "low_idx = y_val_true_sample.idxmin()\n",
    "high_idx = y_val_true_sample.idxmax()\n",
    "mid_idx = y_val_true_sample.iloc[len(y_val_true_sample)//2]\n",
    "\n",
    "example_indices = [low_idx, y_val_true_sample.index[len(y_val_true_sample)//2], high_idx]\n",
    "example_labels = ['Low-Price Property', 'Mid-Price Property', 'High-Price Property']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOCAL SHAP EXPLANATIONS - Individual Property Predictions\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "idx = example_indices[0]\n",
    "sample_idx = X_val_sample.index.get_loc(idx)\n",
    "\n",
    "true_price = np.expm1(y_val.loc[idx])\n",
    "pred_price_log = model.predict(X_val.loc[[idx]])[0]\n",
    "pred_price = np.expm1(pred_price_log)\n",
    "\n",
    "print(f\"\\n{example_labels[0]}:\")\n",
    "print(f\"  True Price: {true_price:,.0f}\")\n",
    "print(f\"  Predicted Price: {pred_price:,.0f}\")\n",
    "print(f\"  Error: {abs(true_price - pred_price):,.0f} ({abs(true_price - pred_price)/true_price*100:.1f}%)\")\n",
    "print(f\"\\n  Top features driving this prediction:\")\n",
    "\n",
    "# Get SHAP values for this example\n",
    "shap_vals_example = shap_values[sample_idx]\n",
    "feature_names = X_val_sample.columns\n",
    "feature_values = X_val_sample.iloc[sample_idx]\n",
    "\n",
    "# Sort by absolute SHAP value\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'value': feature_values,\n",
    "    'shap': shap_vals_example\n",
    "}).sort_values('shap', key=abs, ascending=False).head(5)\n",
    "\n",
    "for _, row in shap_importance.iterrows():\n",
    "    direction = \"increases\" if row['shap'] > 0 else \"decreases\"\n",
    "    print(f\"    - {row['feature']} = {row['value']} \u2192 {direction} price (SHAP: {row['shap']:+.3f})\")\n",
    "\n",
    "# Waterfall plot\n",
    "print(\"\\n  SHAP Waterfall Plot:\")\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_vals_example,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=feature_values,\n",
    "    feature_names=feature_names.tolist()\n",
    "), max_display=10, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "idx = example_indices[1]\n",
    "sample_idx = X_val_sample.index.get_loc(idx)\n",
    "\n",
    "true_price = np.expm1(y_val.loc[idx])\n",
    "pred_price_log = model.predict(X_val.loc[[idx]])[0]\n",
    "pred_price = np.expm1(pred_price_log)\n",
    "\n",
    "print(f\"\\n{example_labels[1]}:\")\n",
    "print(f\"  True Price: {true_price:,.0f}\")\n",
    "print(f\"  Predicted Price: {pred_price:,.0f}\")\n",
    "print(f\"  Error: {abs(true_price - pred_price):,.0f} ({abs(true_price - pred_price)/true_price*100:.1f}%)\")\n",
    "print(f\"\\n  Top features driving this prediction:\")\n",
    "\n",
    "shap_vals_example = shap_values[sample_idx]\n",
    "feature_values = X_val_sample.iloc[sample_idx]\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'value': feature_values,\n",
    "    'shap': shap_vals_example\n",
    "}).sort_values('shap', key=abs, ascending=False).head(5)\n",
    "\n",
    "for _, row in shap_importance.iterrows():\n",
    "    direction = \"increases\" if row['shap'] > 0 else \"decreases\"\n",
    "    print(f\"    - {row['feature']} = {row['value']} \u2192 {direction} price (SHAP: {row['shap']:+.3f})\")\n",
    "\n",
    "print(\"\\n  SHAP Waterfall Plot:\")\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_vals_example,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=feature_values,\n",
    "    feature_names=feature_names.tolist()\n",
    "), max_display=10, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3\n",
    "idx = example_indices[2]\n",
    "sample_idx = X_val_sample.index.get_loc(idx)\n",
    "\n",
    "true_price = np.expm1(y_val.loc[idx])\n",
    "pred_price_log = model.predict(X_val.loc[[idx]])[0]\n",
    "pred_price = np.expm1(pred_price_log)\n",
    "\n",
    "print(f\"\\n{example_labels[2]}:\")\n",
    "print(f\"  True Price: {true_price:,.0f}\")\n",
    "print(f\"  Predicted Price: {pred_price:,.0f}\")\n",
    "print(f\"  Error: {abs(true_price - pred_price):,.0f} ({abs(true_price - pred_price)/true_price*100:.1f}%)\")\n",
    "print(f\"\\n  Top features driving this prediction:\")\n",
    "\n",
    "shap_vals_example = shap_values[sample_idx]\n",
    "feature_values = X_val_sample.iloc[sample_idx]\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'value': feature_values,\n",
    "    'shap': shap_vals_example\n",
    "}).sort_values('shap', key=abs, ascending=False).head(5)\n",
    "\n",
    "for _, row in shap_importance.iterrows():\n",
    "    direction = \"increases\" if row['shap'] > 0 else \"decreases\"\n",
    "    print(f\"    - {row['feature']} = {row['value']} \u2192 {direction} price (SHAP: {row['shap']:+.3f})\")\n",
    "\n",
    "print(\"\\n  SHAP Waterfall Plot:\")\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_vals_example,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=feature_values,\n",
    "    feature_names=feature_names.tolist()\n",
    "), max_display=10, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Business Insights from SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY BUSINESS INSIGHTS FROM MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get mean absolute SHAP values across all samples\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "shap_importance_global = pd.DataFrame({\n",
    "    'feature': X_val_sample.columns,\n",
    "    'importance': mean_abs_shap\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_driver = shap_importance_global.iloc[0]['feature']\n",
    "\n",
    "print(f\"\\n1. PRIMARY PRICE DRIVER: {top_driver}\")\n",
    "if 'ACTUAL_AREA' in top_driver:\n",
    "    print(\"   \u2192 Property size is the #1 determinant of price\")\n",
    "    print(\"   \u2192 Recommendation: Emphasize sqm/sqft in property listings\")\n",
    "elif 'median_price' in top_driver or 'AREA_EN' in top_driver:\n",
    "    print(\"   \u2192 Location premium is the #1 determinant of price\")\n",
    "    print(\"   \u2192 Recommendation: Invest in premium areas for higher returns\")\n",
    "\n",
    "print(\"\\n2. LOCATION EFFECTS:\")\n",
    "location_features = [f for f in shap_importance_global['feature'].head(10) \n",
    "                     if 'AREA' in f or 'PROJECT' in f or 'median_price' in f]\n",
    "if location_features:\n",
    "    print(f\"   \u2192 {len(location_features)} location-related features in top 10\")\n",
    "    print(\"   \u2192 Location is critical for accurate valuation\")\n",
    "\n",
    "print(\"\\n3. TEMPORAL TRENDS:\")\n",
    "time_features = [f for f in shap_importance_global['feature'].head(20) \n",
    "                 if any(x in f.lower() for x in ['year', 'month', 'quarter', 'days'])]\n",
    "if time_features:\n",
    "    print(f\"   \u2192 Time-based features: {time_features[:3]}\")\n",
    "    print(\"   \u2192 Market timing matters for pricing\")\n",
    "\n",
    "print(\"\\n4. PROPERTY CHARACTERISTICS:\")\n",
    "print(\"   \u2192 Size (area), rooms, parking all influence price\")\n",
    "print(\"   \u2192 Larger, well-equipped properties command premium\")\n",
    "\n",
    "print(\"\\n5. MODEL RECOMMENDATIONS FOR PRODUCTION:\")\n",
    "print(\"   \u2713 Model is explainable - can justify predictions to users\")\n",
    "print(\"   \u2713 Location aggregates add significant predictive power\")\n",
    "print(\"   \u2713 Handles unseen properties via fallback to area/type medians\")\n",
    "print(\"   \u2713 Confidence intervals provide uncertainty estimates\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Model Artifact for Production\n",
    "\n",
    "Save everything needed for API deployment in a single artifact file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model artifact\n",
    "save_model_artifact(\n",
    "    model=model,\n",
    "    preprocessing_metadata=preprocessing_metadata,\n",
    "    ci_stats=ci_stats,\n",
    "    feature_importance=feature_importance,\n",
    "    metrics=metrics,\n",
    "    save_path='../models/trained_model.pkl',\n",
    "    area_unit=AREA_UNIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Summary\n",
    "\n",
    "### Model Performance\n",
    "- **Validation R\u00b2:** {metrics['val']['r2']:.4f}\n",
    "- **Validation MAE:** {metrics['val']['mae']:,.0f}\n",
    "- **Test R\u00b2:** {metrics['test']['r2']:.4f}\n",
    "- **Test MAE:** {metrics['test']['mae']:,.0f}\n",
    "\n",
    "### Key Achievements\n",
    "\u2713 Complete EDA with business insights  \n",
    "\u2713 Leak-safe feature engineering  \n",
    "\u2713 CatBoost model with MAE loss  \n",
    "\u2713 Comprehensive evaluation (overall + segmented)  \n",
    "\u2713 SHAP interpretability with business insights  \n",
    "\u2713 Confidence intervals for predictions  \n",
    "\u2713 Production-ready model artifact  \n",
    "\n",
    "### Next Steps\n",
    "1. Deploy FastAPI service using saved artifact\n",
    "2. Write REPORT.md with technical documentation\n",
    "3. Write README.md with setup instructions\n",
    "4. Test end-to-end pipeline\n",
    "\n",
    "---\n",
    "**Analysis Complete!** \u2705"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}